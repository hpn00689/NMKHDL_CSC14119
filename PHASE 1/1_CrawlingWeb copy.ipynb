{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GIAI ĐOẠN 1: LẤY DỮ LIỆU TRÊN WEBSITE\n",
    "---\n",
    "- **Về công cụ**: Sử dụng Selenimum và BeautifulSoup để lấy dữ liệu tự động từ website.\n",
    "- **Điều kiện tiên quyết**: Có cài đặt Chrome drivers và thư viện Selenium.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Import các thư viện cần thiết cho crawling data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thư viện beautifulSoup:\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Thư viện selenium và lấy ra webdriver:\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Thư viện time để sử dụng các hàm liên quan đến thời gian\n",
    "import time\n",
    "\n",
    "# Thư viện xử lý mảng, dataframe: \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Thư viện tải web driver up-to-date\n",
    "import chromedriver_autoinstaller\n",
    "chromedriver_autoinstaller.install()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Tiến hành các bước lấy dữ liệu:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mở trình duyệt Chrome ở chế độ toàn màn hình:\n",
    "browser = webdriver.Chrome()\n",
    "browser.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mở trang chủ của trang cần lấy dữ liệu, ở đây là website Premierleague.com\n",
    "browser.get('https://www.premierleague.com/players')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##### *Giai đoạn 1: Tìm hiểu website và xử lý lần vào đầu tiên*\n",
    "- **Một số lưu ý** khi mới bắt đầu vào website:\n",
    "  - Khi mở website trên lên, mới bắt đầu vào ta sẽ thấy một bảng thông báo là phải **Accept all cookies** thì ta mới <br> có thể thực hiện tiếp trên dữ liệu.\n",
    "  - Có một điểm bất cập ở đây là khi mới bắt đầu vào trang, ta phải đợi nó tải hết một số dữ liệu nên sẽ bị trễ một khoảng thời gian. <br>Với thời gian thực thi của cell tìm và click vào button accept all cookies, nó luôn luôn báo lỗi nên chúng em phải dùng phương thức <br> **time.sleep(3)** để hoãn lại 3 giây chờ web tải hoàn chỉnh rồi sau đó mới có thể nhấn vào được nút.\n",
    "  - 3 giây là thời gian chúng em quan sát được để website tải được hoàn chỉnh ở môi trường lý tưởng (băng thông mạnh,...).\n",
    "- Lỗi trả về và cách sửa lỗi được tham khảo: [Stackoverflow](https://stackoverflow.com/questions/65120003/elementnotinteractableexception-using-selenium-in-python)\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phương thức delay thời gian tải trang.\n",
    "time.sleep(5)   \n",
    "\n",
    "# button_Class: Kiểm tra trên website rồi lấy thủ công về.\n",
    "button_Class = '//button[@class=\"_2hTJ5th4dIYlveipSEMYHH BfdVlAo_cgSVjDUegen0F js-accept-all-close\"]'\n",
    "\n",
    "# Tìm chỗ button_class đó rồi thực hiện click vào (Accept all cookies).\n",
    "browser.find_element(By.XPATH, button_Class).click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "- **Kiểu tải dữ liệu của trang:**\n",
    "  - Giống như Facebook hay Twitter, trang này sẽ tải gần như vô hạn khi tiếp tục kéo xuống dưới cuối.\n",
    "  - Nhóm em thực hiện đoạn code liên tục kéo xuống trang dữ liệu cho đến khi nào nó tải mới, làm hoài như vậy cho đến khi trang đưa ra những dữ liệu cuối cùng.\n",
    "- Kéo trang để lấy hết đoạn html đầy đủ chứa các tag href - chứa toàn bộ link các cầu thủ.\n",
    "- Đoạn code kéo trang được tham khảo tại: [Python forum](https://python-forum.io/thread-20175.html)\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thời gian đợi cuộn tải đữ liệu\n",
    "SCROLL_PAUSE_TIME = 20\n",
    "\n",
    "# Lấy kích thước trang ở cuối.\n",
    "last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    # Cuộn đến cuối trang.\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # Cài đặt thời gian để load trang khi nó mở rộng thêm dữ liệu.\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "    # Tính toán với chiều dài cuộn mới và so sánh với chiều cao trang ban đầu.\n",
    "    new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##### *Giai đoạn 2: Lấy từng chỉ số cầu thủ và đưa ra kết quả cuối cùng.*\n",
    "\n",
    "- Sử dụng song song các thư viện Selenium và thư viện BeautifulSoup để thực hiện việc lấy dữ liệu website chứa thông tin từng cầu thủ.\n",
    "- Vì là dạng tổ chức theo các trang nên việc lấy dữ liệu sẽ rất lâu (chạy 146 phút mới hoàn thành việc lấy hết dữ liệu).\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List chứa danh sách các liên kết đến thông tin cá nhân của một cấu thủ.\n",
    "href_pos_lst = []\n",
    "\n",
    "# Lấy toàn bộ source html trên trang. \n",
    "html_text = browser.page_source\n",
    "soup = BeautifulSoup(html_text)\n",
    "\n",
    "# Đóng trang web: \n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy các thẻ chứa vị trí của các cầu thủ\n",
    "player_pos = soup.find_all('td', 'hide-s')\n",
    "player_pos = player_pos[0::2]\n",
    "\n",
    "for i in range(len(player_pos)):\n",
    "    player_pos[i] = player_pos[i].get_text()\n",
    "\n",
    "player_pos = np.array(player_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy các thẻ chứa các đường dẫn của các cầu thủ\n",
    "player_href = []\n",
    "player_link = soup.find_all('a', 'playerName')\n",
    "\n",
    "# Xử lý link để truy cập vào link nhanh\n",
    "for link in player_link: \n",
    "    raw_link = 'https:' + link.get('href')\n",
    "    true_link = raw_link.replace('overview', 'stats')\n",
    "    player_href.append(true_link)\n",
    "\n",
    "player_href = np.array(player_href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biến 2 mảng thông tin trên thành mảng 2 chiều để dễ xử lý\n",
    "href_pos_lst = np.column_stack((player_pos, player_href))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Làm việc trên danh sách cầu thủ: \n",
    "forward_player_data = [] \n",
    "mid_player_data = [] \n",
    "def_player_data = [] \n",
    "keeper_player_data = [] \n",
    "\n",
    "# Mở lại trình duyệt và tiến hành vào từng cầu thủ\n",
    "browser = webdriver.Chrome()\n",
    "browser.maximize_window()\n",
    "\n",
    "# Vòng lặp duyệt từng link href để vào đượt trang thống kê chỉ số từng cầu thủ.\n",
    "for index, link in enumerate(href_pos_lst):\n",
    "    browser.get(link[1])\n",
    "    # Lúc mới đầu vào sẽ có nút chấp nhận cookies, xử lý nó.\n",
    "    # if (index == 0):\n",
    "    #     time.sleep(4)\n",
    "    #     button_Class = '//button[@class=\"_2hTJ5th4dIYlveipSEMYHH BfdVlAo_cgSVjDUegen0F js-accept-all-close\"]'\n",
    "    #     browser.find_element(By.XPATH, button_Class).click()\n",
    "        \n",
    "    html_text = browser.page_source\n",
    "    soup = BeautifulSoup(html_text)\n",
    "    \n",
    "    # Tìm ra tất cả các thẻ như tên, các mục thống kê rồi lấy các nội dung từ đó.\n",
    "    name = soup.find('div', 'name t-colour').get_text()\n",
    "    if name == 'Ben Chrisene':\n",
    "        continue\n",
    "    topStat = soup.find_all('span', 'allStatContainer')\n",
    "    # Đưa các nội dung vào mảng một chiều như một phần tử của mảng 2 chiều toàn bộ tập dữ liệu.\n",
    "    one_player = [st.get_text().strip() for st in topStat]\n",
    "    one_player.insert(0, name)\n",
    "    print(name, len(one_player))\n",
    "\n",
    "    if (link[0] == 'Goalkeeper'):\n",
    "        keeper_player_data.append(one_player)\n",
    "    elif (link[0] == 'Forward'):    \n",
    "        forward_player_data.append(one_player)\n",
    "    elif (link[0] == 'Midfielder'):   \n",
    "        mid_player_data.append(one_player)     \n",
    "    elif (link[0] == 'Defender'):   \n",
    "        def_player_data.append(one_player)\n",
    "        \n",
    "\n",
    "browser.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfForward = pd.DataFrame(forward_player_data)\n",
    "dfForward.columns = ['Player name', 'Appearances', 'Goals Overview', 'Wins', 'Losses', 'Goals', 'Goals per match',\n",
    "                    'Headed goals', 'Goals with right foot', 'Goals with left foot',\n",
    "                    'Penalties scored', 'Freekicks scored', 'Shots',\n",
    "                    'Shots on target', 'Shooting accuracy', 'Hit woodwork',\n",
    "                    'Big chances missed', 'Assists', 'Passes', 'Passes per match',\n",
    "                    'Big Chances Created', 'Crosses', 'Yellow cards', 'Red cards',\n",
    "                    'Fouls', 'Offsides', 'Tackles', 'Blocked shots',\n",
    "                    'Interceptions', 'Clearances', 'Headed Clearance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfForward.to_csv('forward.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMid = pd.DataFrame(mid_player_data)\n",
    "dfMid.columns = ['Player name', 'Appearances', 'Goals Overview', 'Wins', 'Losses', 'Goals', 'Goals per match',\n",
    "                    'Headed goals', 'Goals with right foot', 'Goals with left foot',\n",
    "                    'Penalties scored', 'Freekicks scored', 'Shots', 'Shots on target', \n",
    "                    'Shooting accuracy %', 'Hit woodwork', 'Big chances missed', 'Assists',\n",
    "                    'Passes', 'Passes per match', 'Big chances created', 'Crosses', 'Cross accuracy %',\n",
    "                    'Through balls', 'Accurate long balls', 'Yellow cards', 'Red cards',\n",
    "                    'Fouls', 'Offsides', 'Tackles', 'Tackle success %', 'Blocked shots',\n",
    "                    'Interceptions', 'Clearances', 'Headed clearances', 'Recoveries', 'Duels won',\n",
    "                    'Duels lost', 'Succesful 50/50s', 'Aerial battles won', 'Aerial battles lost', \n",
    "                    'Errors leading to goal']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMid.to_csv('midfielder_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDef = pd.DataFrame(def_player_data)\n",
    "dfDef.columns = ['Player name', 'Appearances', 'Goals Overview', 'Wins', 'Losses', 'Clean sheets', \n",
    "                'Goals Conceded', 'Tackles', 'Tackle success' , 'Last man tackles',\n",
    "                'Blocked shots', 'Interceptions', 'Clearances', 'Headed Clearance',\n",
    "                'Clearances off line', 'Recoveries', 'Duels won', 'Duels lost', 'Successful 50/50s',\n",
    "                'Aerial battles won', 'Aerial battles lost', 'Own goals', 'Errors leading to goal',\n",
    "                'Assists', 'Passes', 'Passes per match', 'Big Chances Created', 'Crosses',\n",
    "                'Cross accuracy' , 'Through balls', 'Accurate long balls', 'Yellow cards',\n",
    "                'Red cards', 'Fouls', 'Offsides', 'Goals', 'Headed goals', 'Goals with right foot',\n",
    "                'Goals with left foot', 'Hit woodwork']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDef.to_csv('defender_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfKeeper = pd.DataFrame(keeper_player_data)\n",
    "dfKeeper.columns = ['Player name', 'Appearances', 'Clean sheets', 'Wins', 'Losses', 'Saves',\n",
    "            'Penalties Saved', 'Punches', 'High Claims','Catches',\n",
    "            'Sweeper clearances', 'Throw outs', 'Goal Kicks',\n",
    "            'Clean sheets', 'Goals Conceded', 'Errors leading to goal',\n",
    "            'Own goals', 'Yellow cards', 'Red cards', 'Fouls', 'Goals',\n",
    "            'Assists', 'Passes', 'Passes per match', 'Accurate long balls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfKeeper.to_csv('keeper_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Tiến hành đưa dữ liệu về file csv chung:**\n",
    "- Sử dụng thư viện Pandas để đưa mảng chứa các dữ liệu cầu thủ về loại DataFrames của Pandas.\n",
    "- Sau khi chuyển về xong, ta viết các thông tin của Dataframe đó về một file CSV và đánh các tên chỉ số cho các cột dựa trên các thuộc tính lấy được.\n",
    "- Viết lại vào một file csv để tiện dùng cho sau này."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 4. Đánh giá và báo cáo giai đoạn crawling data:\n",
    "- Đánh giá bước lấy dữ liệu: \n",
    "  - **Hạn chế**: \n",
    "    - Việc thực thi các cell lấy dữ liệu mất nhiều thời gian (146 phút) để có thể lấy được toàn bộ dữ liệu. Nguyên nhân một phần là vì các thao tác như sang trang cầu thủ, bấm vào nút thống kê để xem chi tiết, lấy các dữ liệu cần thiết... Và cũng một phần là phải thực hiện kéo trang đến cuối để có thể hiện được hết thông tin.\n",
    "    - Thay đổi trang web vì phải tìm được trang ít lỗi và tĩnh nhất có thể.\n",
    "    - Có một vài trang dữ liệu có định dạng khác format thông thường vì các cầu thủ đó chưa đá nên chưa có dữ liệu, bắt buộc ở bước tiền xử lý dữ liệu thực hiện sau.\n",
    "    - Giai đoạn hoàn chỉnh mất nhiều tuần.\n",
    "  - **Thuận lợi:**\n",
    "    - Không.\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31671a60cee805c34c73116577b485118ff3a75c458d3004d49632c19702ac60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
